import streamlit as st
import sys
import logging

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥transformersåº“
logger.info(f"Python version: {sys.version}")
try:
    import transformers
    logger.info(f"transformers version: {transformers.__version__}")
    # åªå¯¼å…¥å®é™…ä½¿ç”¨çš„ç»„ä»¶ï¼Œç§»é™¤æœªä½¿ç”¨çš„AutoModelForCausalLM
    from transformers import AutoTokenizer, AutoModel
    logger.info("Successfully imported AutoTokenizer and AutoModel")
except Exception as e:
    logger.error(f"Error importing transformers: {e}")
    raise

import torch
import numpy as np
from sklearn.preprocessing import normalize

# æ·»åŠ å›½é™…åŒ–æ”¯æŒ
import json
import os

# åŠ è½½ç¿»è¯‘æ–‡ä»¶
def load_translations():
    translations = {}
    locales_dir = "locales"
    for lang_file in os.listdir(locales_dir):
        if lang_file.endswith(".json"):
            lang = lang_file[:-5]  # ç§»é™¤.jsonåç¼€
            with open(os.path.join(locales_dir, lang_file), "r", encoding="utf-8") as f:
                translations[lang] = json.load(f)
    return translations

# åŠ è½½ç¿»è¯‘
translations = load_translations()

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå¸ƒå±€
st.set_page_config(
    page_title="Transformer ChatGPT å¯è§†åŒ–æ¼”ç¤º",
    page_icon="ğŸ¤–",
    layout="wide"
)

# æ·»åŠ è‡ªå®šä¹‰CSSæ¥éšè—å³ä¸Šè§’çš„éƒ¨ç½²å’Œåˆ†äº«æŒ‰é’®
st.markdown("""<style>
#MainMenu {visibility: hidden;}
header {visibility: hidden;}
footer {visibility: hidden;}
</style>""", unsafe_allow_html=True)

# ä¾§è¾¹æ ï¼šå‚æ•°è®¾ç½®
with st.sidebar:
    # è¯­è¨€é€‰æ‹©å™¨
    lang = st.selectbox(
        "é€‰æ‹©è¯­è¨€ / Select Language",
        options=["zh", "en"],
        index=0,
        format_func=lambda x: "ä¸­æ–‡" if x == "zh" else "English"
    )
    st.markdown("---")

# è·å–ç¿»è¯‘å‡½æ•°
def t(key):
    return translations[lang].get(key, key)

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆä»…åœ¨é¦–æ¬¡è¿è¡Œæ—¶åŠ è½½ï¼‰
@st.cache_resource
def load_models():
    # ä½¿ç”¨æ”¯æŒå¤šè¯­è¨€çš„æ¨¡å‹ï¼Œè§£å†³è‹±æ–‡å•è¯è¢«æ ‡è®°ä¸ºUNKçš„é—®é¢˜
    tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")
    model = AutoModel.from_pretrained("bert-base-multilingual-cased")
    embedding_model = model
    model.eval()
    embedding_model.eval()
    return tokenizer, model, embedding_model

tokenizer, model, embedding_model = load_models()

# è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# ä¸»æ ‡é¢˜
st.title(t("title"))

# ä¾§è¾¹æ ï¼šå‚æ•°è®¾ç½®
with st.sidebar:
    st.header(t("sidebar_header"))
    
    # ç”¨æˆ·è¾“å…¥
    user_input = st.text_input(t("user_input"), "ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£Transformer", max_chars=200, help=t("max_chars_help"))
    
    # ç”Ÿæˆå‚æ•°
    temperature = st.slider(t("temperature"), 0.1, 2.0, 0.7, 0.1, help=t("temperature_help"))
    top_p = st.slider(t("top_p"), 0.1, 1.0, 0.95, 0.05, help=t("top_p_help"))
    top_k = st.slider(t("top_k"), 10, 200, 50, 10, help=t("top_k_help"))
    max_new_tokens = st.slider(t("max_new_tokens"), 50, 500, 200, 50, help=t("max_new_tokens_help"))
    
    # å¤„ç†æ­¥éª¤æ§åˆ¶
    st.header(t("processing_steps"))
    
    # åŸºç¡€å¤„ç†æ­¥éª¤
    step_1 = st.checkbox(t("tokenization"), True)
    step_2 = st.checkbox(t("encoding"), True)
    step_3 = st.checkbox(t("vectorization"), True)
    step_4 = st.checkbox(t("normalization"), True)
    step_5 = st.checkbox(t("correlation"), True)
    step_6 = st.checkbox(t("generation"), True)
    
    # é«˜çº§åŠŸèƒ½åˆ†ç»„
    st.markdown("---")
    st.subheader(t("advanced_features"))
    advanced_features = st.checkbox(t("enable_advanced"), False)
    
    if advanced_features:
        # MCP (Model Context Processing) æ¼”ç¤º
        step_7 = st.checkbox(t("mcp"), True)
        # Skill æ¼”ç¤º
        step_8 = st.checkbox(t("skill"), True)
        # RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) æ¼”ç¤º
        step_9 = st.checkbox(t("rag"), True)
    else:
        step_7 = False
        step_8 = False
        step_9 = False
    
    # å¼€å§‹å¤„ç†æŒ‰é’®
    process_button = st.button(t("process_button"), type="primary")

# ä¸»å†…å®¹åŒºåŸŸ
if process_button:
    # å¯¹è¯å†å²ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥ä¿å­˜ï¼‰
    conversation_history = f"ç”¨æˆ·: {user_input}\nAI: "
    
    # åˆå§‹åŒ–ç»“æœå­˜å‚¨
    results = {}
    
    # æ·»åŠ è¿›åº¦æ¡
    progress_bar = st.progress(0, text=t("processing"))
    total_steps = sum([step_1, step_2, step_3, step_4, step_5, step_6, step_7, step_8, step_9])
    current_step = 0
    
    # 1. åˆ†è¯è¿‡ç¨‹
    if step_1:
        with st.expander(f"ğŸ”¤ {t('tokenization')}", expanded=True):
            st.info(t("tokenization_tip"))
            tokens = tokenizer.tokenize(user_input)
            results['tokens'] = tokens
            st.write(f"{t('original_input')} {user_input}")
            st.write(f"{t('tokenization_result')} {tokens}")
            
            # å¯è§†åŒ–åˆ†è¯
            st.write(f"{t('tokenization_visualization')}")
            for i, token in enumerate(tokens):
                st.code(f"Token {i+1}: '{token}'")
        current_step += 1
        progress_bar.progress(current_step / total_steps, text=f"{t('tokenization_complete')} ({current_step}/{total_steps})")
    
    # 2. ç¼–ç è¿‡ç¨‹
    if step_2:
        with st.expander(f"ğŸ”¢ {t('encoding')}", expanded=True):
            st.info(t("encoding_tip"))
            token_ids = tokenizer.encode(user_input, add_special_tokens=False)
            results['token_ids'] = token_ids
            st.write(f"{t('token_ids')} {token_ids}")
            
            # åˆ†è¯ä¸IDå¯¹åº”å…³ç³»
            if step_1:
                st.write(f"{t('token_id_mapping')}")
                for token, token_id in zip(results['tokens'], token_ids):
                    st.code(f"'{token}' â†’ {token_id}")
        current_step += 1
        progress_bar.progress(current_step / total_steps, text=f"{t('encoding_complete')} ({current_step}/{total_steps})")
    
    # 3. å‘é‡åŒ–è¿‡ç¨‹
    if step_3:
        with st.expander(f"ğŸ“Š {t('vectorization')}", expanded=True):
            st.info(t("vectorization_tip"))
            
            # å°†token IDsè½¬æ¢ä¸ºPyTorchå¼ é‡
            input_ids = torch.tensor([results['token_ids']])
            
            # æ˜¾ç¤ºtokenåˆ°å‘é‡çš„è½¬æ¢æ–¹æ³•
            st.subheader(t("token_to_vector"))
            st.write(t("word_embedding"))
            st.write(t("positional_embedding"))
            st.write(t("layer_norm"))
            st.write(t("multi_head_attention"))
            st.write(t("feed_forward"))
            
            # å¯è§†åŒ–ç‚¹ç§¯è®¡ç®—è¿‡ç¨‹
            st.subheader(t("dot_product_visualization"))
            st.write(t("dot_product_desc"))
            
            # ç®€å•çš„ç‚¹ç§¯ç¤ºä¾‹
            vec_a = np.array([0.5, 0.7, 0.2])
            vec_b = np.array([0.3, 0.6, 0.9])
            dot_product = np.dot(vec_a, vec_b)
            
            col1, col2, col3 = st.columns(3)
            col1.write(f"{t('vector_a')}")
            col1.write(vec_a)
            col2.write(f"{t('vector_b')}")
            col2.write(vec_b)
            col3.write(f"{t('dot_product_result')}")
            col3.write(f"{dot_product:.4f}")
            
            st.write(t("dot_product_calc"))
            
            # Softmaxè¿‡ç¨‹æè¿°å’Œå¯è§†åŒ–
            st.subheader(t("softmax_activation"))
            st.info(t("softmax_tip"))
            
            # ç®€å•çš„softmaxç¤ºä¾‹
            logits = np.array([2.0, 1.0, 0.1])
            exp_logits = np.exp(logits)
            softmax_probs = exp_logits / np.sum(exp_logits)
            
            st.write(f"{t('input_logits')} {logits}")
            st.write(f"{t('exponential_transform')} {exp_logits}")
            st.write(f"{t('sum_result')} {np.sum(exp_logits):.4f}")
            st.write(f"{t('softmax_probs')} {softmax_probs}")
            st.write(f"{t('probability_sum')} {np.sum(softmax_probs):.4f}")
            
            # å¯è§†åŒ–softmaxæ›²çº¿
            st.bar_chart({
                'Logits': logits,
                'Softmax Probabilities': softmax_probs
            })
            
            # è·å–æ¨¡å‹çš„éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æƒé‡
            with torch.no_grad():
                # è®¾ç½®output_attentions=Trueä»¥è·å–æ³¨æ„åŠ›æƒé‡
                outputs = embedding_model(input_ids, output_attentions=True)
                last_hidden_state = outputs.last_hidden_state
                attentions = outputs.attentions
                sentence_vector = last_hidden_state.mean(dim=1).squeeze().numpy()
            
            results['sentence_vector'] = sentence_vector
            results['attentions'] = attentions
            
            # çªå‡ºæ³¨æ„åŠ›å…³ç³»
            st.subheader(t("attention_visualization"))
            st.info(t("attention_tip"))
            
            if attentions is not None and len(tokens) > 0:
                # è·å–æœ€åä¸€å±‚çš„æ³¨æ„åŠ›æƒé‡
                last_layer_attention = attentions[-1].squeeze(0).numpy()  # å½¢çŠ¶: (num_heads, seq_len, seq_len)
                num_heads = last_layer_attention.shape[0]
                
                st.write(f"{t('attention_heads')} {num_heads}")
                
                # é€‰æ‹©ä¸€ä¸ªæ³¨æ„åŠ›å¤´è¿›è¡Œå¯è§†åŒ–ï¼ˆè¿™é‡Œé€‰æ‹©ç¬¬0ä¸ªï¼‰
                attention_head = 0
                attention_matrix = last_layer_attention[attention_head]
                
                # ç¡®ä¿tokensæ•°é‡ä¸æ³¨æ„åŠ›çŸ©é˜µç»´åº¦ä¸€è‡´
                seq_len = attention_matrix.shape[0]
                if len(tokens) < seq_len:
                    # å¦‚æœtokensæ•°é‡ä¸è¶³ï¼Œç”¨ç©ºå­—ç¬¦ä¸²å¡«å……
                    display_tokens = tokens + ["[PAD]"] * (seq_len - len(tokens))
                else:
                    display_tokens = tokens[:seq_len]
                
                # è®¡ç®—æ˜¾ç¤ºçš„æ³¨æ„åŠ›å¤´ç¼–å·ï¼ˆä»1å¼€å§‹ï¼‰
                head_num = attention_head + 1
                
                # æ˜¾ç¤ºæ³¨æ„åŠ›çƒ­åŠ›å›¾
                st.write(t("attention_matrix").format(head_num=head_num))
                st.write(t("attention_cell_desc"))
                
                # åˆ›å»ºæ³¨æ„åŠ›æƒé‡DataFrameç”¨äºçƒ­åŠ›å›¾
                import pandas as pd
                attention_df = pd.DataFrame(attention_matrix, index=display_tokens, columns=display_tokens)
                st.dataframe(attention_df.style.background_gradient(cmap='viridis', axis=None))
                
                # æ˜¾ç¤ºæ³¨æ„åŠ›æƒé‡çš„æœ€å¤§å€¼
                max_attention = attention_matrix.max()
                max_pos = np.unravel_index(attention_matrix.argmax(), attention_matrix.shape)
                # è®¡ç®—æ˜¾ç¤ºçš„tokenç¼–å·ï¼ˆä»1å¼€å§‹ï¼‰
                token_from = max_pos[0] + 1
                token_to = max_pos[1] + 1
                st.write(t("max_attention").format(max_attn=max_attention, token_from=token_from, token_to=token_to))
            
            st.subheader(t("final_vector"))
            st.write(f"{t('vector_dimension')} {sentence_vector.shape}")
            st.write(f"{t('vector_first_20')} {sentence_vector[:20]}")
            
            # å‘é‡ç»Ÿè®¡ä¿¡æ¯
            st.write(f"{t('vector_stats')}")
            col1, col2, col3, col4 = st.columns(4)
            col1.metric(t("min_value"), f"{np.min(sentence_vector):.4f}")
            col2.metric(t("max_value"), f"{np.max(sentence_vector):.4f}")
            col3.metric(t("mean_value"), f"{np.mean(sentence_vector):.4f}")
            col4.metric(t("std_value"), f"{np.std(sentence_vector):.4f}")
        current_step += 1
        progress_bar.progress(current_step / total_steps, text=f"{t('vectorization_complete')} ({current_step}/{total_steps})")
    
    # 4. å½’ä¸€åŒ–è¿‡ç¨‹
    if step_4:
        with st.expander(f"ğŸ“ {t('normalization')}", expanded=True):
            st.info(t("normalization_tip"))
            sentence_vector = results['sentence_vector']
            normalized_vector = normalize([sentence_vector], norm='l2')[0]
            results['normalized_vector'] = normalized_vector
            
            st.write(f"{t('norm_before')} {np.linalg.norm(sentence_vector):.6f}")
            st.write(f"{t('norm_after')} {np.linalg.norm(normalized_vector):.6f}")
            st.write(f"{t('normalized_vector')} {normalized_vector[:20]}")
            
            # å¯è§†åŒ–å½’ä¸€åŒ–å‰åçš„å‘é‡å˜åŒ–
            st.write(f"{t('norm_comparison')}")
            col1, col2 = st.columns(2)
            col1.write(f"{t('before_normalization')}")
            col1.line_chart(sentence_vector[:50])
            col2.write(f"{t('after_normalization')}")
            col2.line_chart(normalized_vector[:50])
        current_step += 1
        progress_bar.progress(current_step / total_steps, text=f"{t('normalization_complete')} ({current_step}/{total_steps})")
    
    # 5. ç›¸å…³æ€§è®¡ç®—ï¼ˆä¸é¢„è®¾çš„ç¤ºä¾‹å‘é‡ï¼‰
    if step_5:
        with st.expander(f"ğŸ”— {t('correlation')}", expanded=True):
            st.info(t("correlation_tip"))
            # é¢„è®¾ä¸€äº›ç¤ºä¾‹å‘é‡ç”¨äºç›¸å…³æ€§è®¡ç®—
            example_vectors = {
                "é—®å€™": np.random.randn(768),
                "æŠ€æœ¯": np.random.randn(768),
                "å¨±ä¹": np.random.randn(768),
                "æ•™è‚²": np.random.randn(768),
                "å¥åº·": np.random.randn(768)
            }
            
            # å½’ä¸€åŒ–ç¤ºä¾‹å‘é‡
            for key in example_vectors:
                example_vectors[key] = normalize([example_vectors[key]], norm='l2')[0]
            
            # è®¡ç®—ç›¸å…³æ€§
            similarities = {}
            for key, vec in example_vectors.items():
                sim = cosine_similarity(results['normalized_vector'], vec)
                similarities[key] = sim
            
            st.write(f"{t('correlation_with_categories')}")
            for key, sim in similarities.items():
                st.progress(float((sim + 1) / 2), text=f"{key}: {sim:.4f}")
            
            # æ˜¾ç¤ºæœ€é«˜ç›¸å…³æ€§
            most_relevant = max(similarities, key=similarities.get)
            similarity_value = similarities[most_relevant]
            st.write(t("most_relevant").format(most_relevant=most_relevant, similarity_value=similarity_value))
        current_step += 1
        progress_bar.progress(current_step / total_steps, text=f"{t('correlation_complete')} ({current_step}/{total_steps})")
    
    # 6. æ–‡æœ¬åˆ†ç±»ï¼ˆBERTæ˜¯ç¼–ç å™¨æ¨¡å‹ï¼Œç”¨äºåˆ†ç±»è€Œä¸æ˜¯ç”Ÿæˆï¼‰
    if step_6:
        with st.expander(f"ğŸ·ï¸ {t('generation')}", expanded=True):
            st.info(t("classification_tip"))
            st.write(f"{t('bert_analysis')}")
            st.info(t("bert_info"))
            
            # æ·»åŠ Transformeræ¶æ„ç±»å‹è®²è§£
            st.subheader("Transformer Architecture Types")
            st.markdown("""
            Transformeræ¨¡å‹ä¸»è¦åˆ†ä¸ºä¸‰ç§æ¶æ„ç±»å‹ï¼Œæ¯ç§æ¶æ„æœ‰ä¸åŒçš„åº”ç”¨åœºæ™¯ï¼š
            
            ### 1. Encoder-only Architecture
            **ä»£è¡¨æ¨¡å‹**: BERT, RoBERTa, ALBERT, DistilBERT
            **ç‰¹ç‚¹**:
            - ä»…åŒ…å«Transformerç¼–ç å™¨éƒ¨åˆ†
            - åŒå‘æ³¨æ„åŠ›ï¼Œèƒ½åŒæ—¶çœ‹åˆ°ä¸Šä¸‹æ–‡ä¿¡æ¯
            - é€‚åˆç†è§£ç±»ä»»åŠ¡
            **åº”ç”¨åœºæ™¯**:
            - æ–‡æœ¬åˆ†ç±»
            - å‘½åå®ä½“è¯†åˆ«
            - æƒ…æ„Ÿåˆ†æ
            - æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
            - ä¿¡æ¯æ£€ç´¢
            
            ### 2. Decoder-only Architecture
            **ä»£è¡¨æ¨¡å‹**: GPTç³»åˆ—, Llama, Mistral, Gemma
            **ç‰¹ç‚¹**:
            - ä»…åŒ…å«Transformerè§£ç å™¨éƒ¨åˆ†
            - å•å‘æ³¨æ„åŠ›ï¼Œåªèƒ½çœ‹åˆ°ä¹‹å‰çš„ä¿¡æ¯
            - é€‚åˆç”Ÿæˆç±»ä»»åŠ¡
            **åº”ç”¨åœºæ™¯**:
            - æ–‡æœ¬ç”Ÿæˆ
            - å¯¹è¯ç³»ç»Ÿ
            - æ•…äº‹åˆ›ä½œ
            - ä»£ç ç”Ÿæˆ
            - è‡ªåŠ¨å†™ä½œ
            
            ### 3. Encoder-Decoder Architecture
            **ä»£è¡¨æ¨¡å‹**: T5, BART, mT5, Pegasus
            **ç‰¹ç‚¹**:
            - åŒ…å«å®Œæ•´çš„ç¼–ç å™¨å’Œè§£ç å™¨
            - ç¼–ç å™¨å¤„ç†è¾“å…¥ï¼Œè§£ç å™¨ç”Ÿæˆè¾“å‡º
            - é€‚åˆåºåˆ—åˆ°åºåˆ—ä»»åŠ¡
            **åº”ç”¨åœºæ™¯**:
            - æœºå™¨ç¿»è¯‘
            - æ–‡æœ¬æ‘˜è¦
            - é—®ç­”ç³»ç»Ÿ
            - æ–‡æœ¬æ”¹å†™
            - è¯­éŸ³è¯†åˆ«
            
            ### Architecture Comparison
            | Architecture | Key Features | Typical Tasks | Representative Models |
            |--------------|--------------|---------------|------------------------|
            | Encoder-only | Bidirectional attention | Understanding tasks | BERT, RoBERTa |
            | Decoder-only | Unidirectional attention | Generation tasks | GPT, Llama |
            | Encoder-Decoder | Both encoder and decoder | Sequence-to-sequence tasks | T5, BART |
            
            This demo uses BERT, an encoder-only model, which is why it excels at understanding and classification tasks but doesn't support text generation like GPT models.
            """)
            
            st.write(f"{t('vector_applications')}")
            st.write(t("similarity_calc"))
            st.write(t("text_classification"))
            st.write(t("information_retrieval"))
            st.write(t("clustering"))
            st.write(t("recommendation"))
        current_step += 1
        progress_bar.progress(current_step / total_steps, text=f"{t('generation_complete')} ({current_step}/{total_steps})")
    
    # 7. MCP (Model Context Processing) æ¼”ç¤º
    if step_7:
        with st.expander(f"ğŸ§  {t('mcp')}", expanded=True):
            st.info(t("mcp_tip"))
            
            st.subheader(t("mcp_process"))
            
            # å±•ç¤ºä¸Šä¸‹æ–‡å¤„ç†çš„ä¸åŒé˜¶æ®µ
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.markdown(t("context_acquisition"))
                st.write(t("context_acquisition_desc"))
            
            with col2:
                st.markdown(t("context_optimization"))
                st.write(t("context_optimization_desc"))
            
            with col3:
                st.markdown(t("context_injection"))
                st.write(t("context_injection_desc"))
            
            # å¯è§†åŒ–ç¤ºä¾‹
            st.subheader(t("mcp_example"))
            
            # åŸå§‹ä¸Šä¸‹æ–‡
            original_context = "ç”¨æˆ·: ä½ å¥½\nAI: ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„ï¼Ÿ\nç”¨æˆ·: æˆ‘æƒ³äº†è§£Transformer\nAI: Transformeræ˜¯ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¨¡å‹...\nç”¨æˆ·: é‚£å®ƒå’ŒRNNæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
            
            # ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡
            optimized_context = "ä»»åŠ¡: è§£é‡ŠTransformerå’ŒRNNçš„åŒºåˆ«\nç›¸å…³å†å²: ç”¨æˆ·è¯¢é—®äº†Transformerçš„åŸºæœ¬ä¿¡æ¯\nå½“å‰æŸ¥è¯¢: å®ƒå’ŒRNNæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown(f"{t('original_context')}")
                st.code(original_context, language="text")
            with col2:
                st.markdown(f"{t('optimized_context')}")
                st.code(optimized_context, language="text")
            
            st.write(f"{t('mcp_advantages')}")
            st.write(t("mcp_advantage1"))
            st.write(t("mcp_advantage2"))
            st.write(t("mcp_advantage3"))
            st.write(t("mcp_advantage4"))
        current_step += 1
        progress_bar.progress(current_step / total_steps, text=f"{t('mcp_complete')} ({current_step}/{total_steps})")
    
    # 8. Skill (æŠ€èƒ½è°ƒç”¨) æ¼”ç¤º
    if step_8:
        with st.expander(f"ğŸ› ï¸ {t('skill')}", expanded=True):
            st.info(t("skill_tip"))
            
            st.subheader(t("skill_process"))
            
            # æŠ€èƒ½è°ƒç”¨çš„åŸºæœ¬æ­¥éª¤
            skill_steps = [
                {"name": t("intent_recognition"), "desc": t("intent_recognition_desc")},
                {"name": t("skill_selection"), "desc": t("skill_selection_desc")},
                {"name": t("parameter_extraction"), "desc": t("parameter_extraction_desc")},
                {"name": t("skill_execution"), "desc": t("skill_execution_desc")},
                {"name": t("result_integration"), "desc": t("result_integration_desc")}
            ]
            
            for step in skill_steps:
                with st.container(border=True):
                    st.markdown(f"{step['name']}")
                    st.write(step['desc'])
            
            # æŠ€èƒ½ç¤ºä¾‹
            st.subheader(t("skill_examples"))
            
            # ç¤ºä¾‹æŠ€èƒ½åˆ—è¡¨
            skills = {
                "è®¡ç®—å™¨": "æ‰§è¡Œæ•°å­¦è®¡ç®—",
                "å¤©æ°”æŸ¥è¯¢": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
                "æ—¥æœŸæ—¶é—´": "è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´",
                "ç¿»è¯‘": "å°†æ–‡æœ¬ç¿»è¯‘ä¸ºæŒ‡å®šè¯­è¨€",
                "æœç´¢å¼•æ“": "æœç´¢äº’è”ç½‘è·å–ç›¸å…³ä¿¡æ¯"
            }
            
            st.write(f"{t('available_skills')}")
            for skill_name, skill_desc in skills.items():
                st.code(f"{skill_name}: {skill_desc}")
            
            # æ¼”ç¤ºæŠ€èƒ½è°ƒç”¨è¿‡ç¨‹
            st.subheader(t("skill_demonstration"))
            
            # æ¨¡æ‹ŸæŠ€èƒ½è°ƒç”¨
            user_request = "100çš„å¹³æ–¹æ ¹æ˜¯å¤šå°‘ï¼Ÿ"
            skill_selected = "è®¡ç®—å™¨"
            parameters = {"expression": "sqrt(100)"}
            skill_result = 10.0
            final_response = f"100çš„å¹³æ–¹æ ¹æ˜¯{skill_result}ã€‚"
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown(f"{t('user_request')}")
                st.code(user_request)
                st.markdown(f"{t('selected_skill')}")
                st.code(skill_selected)
                st.markdown(f"{t('skill_parameters')}")
                st.code(parameters)
            with col2:
                st.markdown(f"{t('skill_result')}")
                st.code(str(skill_result))
                st.markdown(f"{t('final_response')}")
                st.code(final_response)
        current_step += 1
        progress_bar.progress(current_step / total_steps, text=f"{t('skill_complete')} ({current_step}/{total_steps})")
    
    # 9. RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) æ¼”ç¤º
    if step_9:
        with st.expander(f"ğŸ” {t('rag')}", expanded=True):
            st.info(t("rag_tip"))
            
            st.subheader(t("rag_process"))
            
            # RAGçš„æ ¸å¿ƒç»„ä»¶å’Œæµç¨‹
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown(t("rag_components"))
                st.write(t("retriever"))
                st.write(t("generator"))
                st.write(t("document_library"))
                st.write(t("indexer"))
            
            with col2:
                st.markdown(t("rag_workflow"))
                st.write(t("rag_workflow1"))
                st.write(t("rag_workflow2"))
                st.write(t("rag_workflow3"))
                st.write(t("rag_workflow4"))
                st.write(t("rag_workflow5"))
            
            # å¯è§†åŒ–RAGæµç¨‹
            st.subheader(t("rag_visualization"))
            
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„RAGæµç¨‹å›¾
            st.markdown('''```
ç”¨æˆ·æŸ¥è¯¢ â†’ å‘é‡è½¬æ¢ â†’ æ–‡æ¡£æ£€ç´¢ â†’ ç»“æœæ•´åˆ â†’ æ¨¡å‹ç”Ÿæˆ â†’ æœ€ç»ˆå“åº”
          â†‘           â†‘
          |           |
   åµŒå…¥æ¨¡å‹       æ–‡æ¡£åº“
            ```''')
            
            # ç¤ºä¾‹æ¼”ç¤º
            st.subheader(t("rag_example"))
            
            # æ¨¡æ‹ŸRAGè¿‡ç¨‹
            rag_query = "Transformerçš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ"
            
            # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
            retrieved_docs = [
                {"title": "Transformeræ³¨æ„åŠ›æœºåˆ¶è¯¦è§£", "content": "Transformerä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥è®¡ç®—è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªä½ç½®ä¸å…¶ä»–ä½ç½®çš„å…³è”ç¨‹åº¦..."},
                {"title": "æ³¨æ„åŠ›æœºåˆ¶åœ¨NLPä¸­çš„åº”ç”¨", "content": "æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†æ¯ä¸ªä½ç½®æ—¶å…³æ³¨è¾“å…¥åºåˆ—ä¸­çš„ç›¸å…³ä½ç½®..."},
                {"title": "Transformeræ¶æ„è§£æ", "content": "å¤šå¤´æ³¨æ„åŠ›æ˜¯Transformerçš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒå…è®¸æ¨¡å‹ä»ä¸åŒè§’åº¦å…³æ³¨è¾“å…¥ä¿¡æ¯..."}
            ]
            
            st.write(f"{t('rag_query')}")
            st.code(rag_query)
            
            st.write(f"{t('retrieved_documents')}")
            for doc in retrieved_docs:
                with st.container(border=True):
                    st.markdown(f"### {doc['title']}")
                    st.write(doc['content'][:150] + "...")
            
            st.write(f"{t('rag_advantages')}")
            st.write(t("rag_advantage1"))
            st.write(t("rag_advantage2"))
            st.write(t("rag_advantage3"))
            st.write(t("rag_advantage4"))
        current_step += 1
        progress_bar.progress(current_step / total_steps, text=f"{t('rag_complete')} ({current_step}/{total_steps})")
    
    # å¤„ç†å®Œæˆ
    progress_bar.progress(1.0, text=t("processing_completed"))
    st.success(t("all_steps_completed"))

# é¡µè„š
st.markdown("---")
st.markdown("**Transformer Visual Demo - Based on BERT-base-multilingual-cased Model**")
